{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='', client_secret='', user_agent='reddit scrap')\n",
    "flairs = []\n",
    "hot_posts = reddit.subreddit('india').new(limit=1000)\n",
    "for post in hot_posts:\n",
    "    flair = post.link_flair_text\n",
    "    flairs.append(flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'AskIndia',\n",
       "  'Business/Finance',\n",
       "  'CAA-NRC-NPR',\n",
       "  'Coronavirus',\n",
       "  'Food',\n",
       "  'Non-Political',\n",
       "  None,\n",
       "  'Old',\n",
       "  'Photography',\n",
       "  'Policy/Economy',\n",
       "  'Politics',\n",
       "  'Scheduled',\n",
       "  'Science/Technology',\n",
       "  'Sports',\n",
       "  '| Repost |'},\n",
       " 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(flairs), len(set(flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There are in total of 19 distinct flairs from the first 1000 posts from subreddit india. Doing classification considering all flairs will not be good idea because:\n",
    "        1. Considering more classes will make our task much more difficult.\n",
    "        2. With more classes maintaing balanced data will be a huge problem.\n",
    "    So, I have decided to narrow down the no. of classes based on name and current socio-political scenario. The total no. of classes that I have considered now is 10 and these are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appropriate flairs\n",
    "flairs = ['Coronavirus',\n",
    " 'Politics',\n",
    " 'Photography',\n",
    " 'Policy/Economy',\n",
    " 'Non-Political',\n",
    " 'AskIndia',\n",
    " 'Business/Finance',\n",
    " 'Science/Technology',\n",
    " 'Sports',\n",
    " 'CAA-NRC-NPR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   In the above cells, I have used [praw](https://github.com/praw-dev/praw) library to scrape data from reddit. But the problem with this library is that it uses reddit official API to fetch data because of which we can only fetch top 1000 posts, but for training we will need way more than 1000. So, I decided to go with something else:\n",
    "   [PushshiftAPI](https://github.com/pushshift/api) is good option and [psaw](https://github.com/dmarx/psaw) is a wrapper for this API, which I will be using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "result = list(api.search_submissions(subreddit='india',\n",
    "                            filter=['title','full_link', 'selftext', 'link_flair_text'],\n",
    "                            limit=150000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission(created_utc=1585895305, full_link='https://www.reddit.com/r/india/comments/fu38te/india_code_digital_repository_of_central_and/', link_flair_text='Politics', selftext='', title='India Code | Digital Repository of Central and State Acts', created=1585875505.0, d_={'created_utc': 1585895305, 'full_link': 'https://www.reddit.com/r/india/comments/fu38te/india_code_digital_repository_of_central_and/', 'link_flair_text': 'Politics', 'selftext': '', 'title': 'India Code | Digital Repository of Central and State Acts', 'created': 1585875505.0})\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "{'created_utc': 1585895305, 'full_link': 'https://www.reddit.com/r/india/comments/fu38te/india_code_digital_repository_of_central_and/', 'link_flair_text': 'Politics', 'selftext': '', 'title': 'India Code | Digital Repository of Central and State Acts', 'created': 1585875505.0}\n"
     ]
    }
   ],
   "source": [
    "print(result[9999])\n",
    "print(\"---\"*39)\n",
    "print(result[9999][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    submission = reddit.submission(url=url)\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    try:\n",
    "        return submission.comments[0].body\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Initially, I tried to get the top comment for analysis but the comment can't be directly scraped, so the way to get the comment is shown in the above cell. Now the problem with extracting comments is that It was taking a lot of time approximately for 100000 posts, the estimated time was more than 7 hours. So, I decided to parallelise the process, but still it was taking more than 4 hours. And sending so many requests to reddit API was also difficult, so it was throwing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'title':[],'full_link':[], 'selftext':[], 'link_flair_text':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e871c9b88844df69a60859b4b26a9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress Bar for Insertion in disk', max=100000.0, style=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ind, res in tqdm.tqdm_notebook(enumerate(result), total = 100000, desc = \"Progress Bar for Insertion in disk\"):\n",
    "    \n",
    "    result_dict[\"title\"].append(res[-1][\"title\"])\n",
    "    result_dict[\"full_link\"].append(res[-1][\"full_link\"])\n",
    "    \n",
    "    if \"selftext\" in res[-1].keys():\n",
    "        result_dict[\"selftext\"].append(res[-1][\"selftext\"]) \n",
    "    else:\n",
    "        result_dict[\"selftext\"].append(None) \n",
    "    \n",
    "    if \"link_flair_text\" in res[-1].keys():\n",
    "        result_dict[\"link_flair_text\"].append(res[-1][\"link_flair_text\"]) \n",
    "    else:\n",
    "        result_dict[\"link_flair_text\"].append(None) \n",
    "    \n",
    "#     result_dict[\"comment\"].append(comment(res[-1][\"full_link\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(result_dict)\n",
    "df.to_csv(\"~/Documents/india_reddit15.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parallelising the above function to scrape comments which at last didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocess import Process, Manager\n",
    "\n",
    "# def f(result_dict, res):\n",
    "#     result_dict[\"title\"].append(res[-1][\"title\"])\n",
    "#     result_dict[\"full_link\"].append(res[-1][\"full_link\"])\n",
    "    \n",
    "#     if \"selftext\" in res[-1].keys():\n",
    "#         result_dict[\"selftext\"].append(res[-1][\"selftext\"]) \n",
    "#     else:\n",
    "#         result_dict[\"selftext\"].append(None) \n",
    "    \n",
    "#     if \"link_flair_text\" in res[-1].keys():\n",
    "#         result_dict[\"link_flair_text\"].append(res[-1][\"link_flair_text\"]) \n",
    "#     else:\n",
    "#         result_dict[\"link_flair_text\"].append(None) \n",
    "    \n",
    "#     result_dict[\"comment\"].append(comment(res[-1][\"full_link\"]))\n",
    "    \n",
    "\n",
    "# manager = Manager()\n",
    "# result_dict = manager.dict({'title':[],'full_link':[], 'selftext':[], 'link_flair_text':[], \"comment\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# job = [Process(target=f, args=(result_dict, res)) for res in tqdm.tqdm_notebook(result, total = 100000, desc = \"Progress Bar for Insertion in disk\")]\n",
    "# _ = [p.start() for p in job]\n",
    "# _ = [p.join() for p in job]\n",
    "# # _ = [p.join() for p in job]\n",
    "\n",
    "\n",
    "# print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
