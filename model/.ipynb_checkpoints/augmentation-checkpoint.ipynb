{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# installing necessary libraries to perform this task\n",
    "!pip install torch torchvision transformers rasa==1.7.0 input_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import ipywidgets as widgets\n",
    "import requests, os\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "\n",
    "from rasa.nlu.training_data import TrainingData,Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download model\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "model_class_file_id = '1N1kn2b7i2ND7eNefzyJM-k13IM8tqZvr'\n",
    "checkpoint_file_id = '1G0nwXlvzGsb8Ar-OAnYBQKFvY97WMzBy'\n",
    "model_class_destination = 'model.py'\n",
    "checkpoint_destination = 'model.zip'\n",
    "checkpoint_unzipped_destination = 'package_models'\n",
    "\n",
    "if not os.path.exists(checkpoint_unzipped_destination):\n",
    "    download_file_from_google_drive(checkpoint_file_id, checkpoint_destination)\n",
    "    !unzip {checkpoint_destination}\n",
    "\n",
    "if not os.path.exists(model_class_destination):\n",
    "    download_file_from_google_drive(model_class_file_id, model_class_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "from model import ParaphraseModel\n",
    "model_path = 'package_models/lm_finetune_8/checkpoint-56000/'\n",
    "\n",
    "complete_td = TrainingData()\n",
    "model = ParaphraseModel(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our dataset\n",
    "train_df = pd.read_csv('../input/data-divide/reddit_data1.csv')\n",
    "\n",
    "# dropping rows having null values\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# creating a label column to encode our text labels to no.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df[\"flair\"])\n",
    "train_df[\"label\"] = le.transform(train_df[\"flair\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'text':[], \"label\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i have created a scheme for augmentation. Through this scheme, not for every sentence paraphrases\n",
    "# will be calculated. It will depend on label of that text. So, for label which have a very low percentages in \n",
    "# in our data, for them the augmentation will be more aggressive. But for labels which have enough data,\n",
    "# we are not augmenting.\n",
    "for ind, i in train_df.iterrows():\n",
    "    if (i[\"label\"] == 6) or (i[\"label\"] == 4):\n",
    "        text = model.get_paraphrases(i[\"dirty_text\"], 5, \"\")\n",
    "        result_dict[\"text\"].extend([text[1], i[\"dirty_text\"], text[4]])\n",
    "        result_dict[\"label\"].extend([i[\"label\"], i[\"label\"], i[\"label\"]])\n",
    "        \n",
    "    if i[\"label\"] == 1:\n",
    "        if np.random.random()<=0.84:\n",
    "            text = model.get_paraphrases(i[\"dirty_text\"], 5, \"\")\n",
    "            result_dict[\"text\"].extend([text[1],text[4]])\n",
    "            result_dict[\"label\"].extend([i[\"label\"], i[\"label\"], i[\"label\"]])\n",
    "        result_dict[\"text\"].append(i[\"dirty_text\"])\n",
    "        \n",
    "    if i[\"label\"] == 2:\n",
    "        if np.random.random()<=0.6:\n",
    "            text = model.get_paraphrases(i[\"dirty_text\"], 5, \"\")\n",
    "            result_dict[\"text\"].extend([text[1]])\n",
    "            result_dict[\"label\"].extend([i[\"label\"], i[\"label\"]])\n",
    "        result_dict[\"text\"].append(i[\"dirty_text\"])\n",
    "    \n",
    "    if i[\"label\"] == 0:\n",
    "        if np.random.random()<=0.18:\n",
    "            text = model.get_paraphrases(i[\"dirty_text\"], 5, \"\")\n",
    "            result_dict[\"text\"].extend([text[1]])\n",
    "            result_dict[\"label\"].extend([i[\"label\"], i[\"label\"]])\n",
    "        result_dict[\"text\"].append(i[\"dirty_text\"])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(result_dict)\n",
    "df.to_csv(\"augmented_data1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
