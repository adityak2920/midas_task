{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing different libraries\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>dirty_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top comments toi article drop us oil prices</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Top comments on a TOI article about the drop i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hacking networking security 2 books 1 hacking ...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Hacking: Networking and Security (2 Books in 1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zakir khan irfan junejo live instagram session...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Zakir Khan and Irfan Junejo live Instagram Ses...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cursing quentin tarantino movie</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Cursing In A Quentin Tarantino Movie</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          flair  \\\n",
       "0        top comments toi article drop us oil prices  Non-Political   \n",
       "1                                       disappointed       Politics   \n",
       "2  hacking networking security 2 books 1 hacking ...  Non-Political   \n",
       "3  zakir khan irfan junejo live instagram session...  Non-Political   \n",
       "4                    cursing quentin tarantino movie  Non-Political   \n",
       "\n",
       "                                          dirty_text  label  \n",
       "0  Top comments on a TOI article about the drop i...      3  \n",
       "1                                       Disappointed      5  \n",
       "2  Hacking: Networking and Security (2 Books in 1...      3  \n",
       "3  Zakir Khan and Irfan Junejo live Instagram Ses...      3  \n",
       "4               Cursing In A Quentin Tarantino Movie      3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading our dataset\n",
    "train_df = pd.read_csv('../input/midas-task/reddit_data.csv')\n",
    "\n",
    "# dropping rows having null values\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# creating a label column to encode our text labels to no.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df[\"flair\"])\n",
    "train_df[\"label\"] = le.transform(train_df[\"flair\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have two columns of texts \"text\" and \"dirty_text\" which we can use for training. The type of vectorizer       and tokenizer which we will be using further for modelling and creating pipeline will not need cleaned text. The text will be cleaned during vectorization itself, so we will be using \"dirty_text\" column for training instead of \"text\" beacuse it is previously cleaned.\n",
    "    \n",
    "> Note: Actually, I have tried training with both \"dirty_text\" and \"text\" both to validate my above assumption. The results will be shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data and using \"dirty_text\" for training\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_df.dirty_text, train_df.label, \n",
    "                                                  stratify=train_df.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61856,)\n",
      "(15465,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for Modelling\n",
    "We will be using two types of vectorizer:\n",
    "1. TfidfVectorizer\n",
    "2. CountVectorizer\n",
    "We will be using our own tokenizer from NLTK.\n",
    "    \n",
    "   ## Models\n",
    "   1. Logistic Regression\n",
    "   2. Multinomial Naive Bayes\n",
    "   3. Support Vector Machines\n",
    "   4. XGBoost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tokenization, we are also using stemming to reduce no. of unique tokens\n",
    "# And we are using Porter Stemmer from NLTK for stemming\n",
    "# def wordtokenize(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     stems = []\n",
    "#     for item in tokens:\n",
    "#         stems.append(PorterStemmer().stem(item))\n",
    "#     return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectors.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the TF-IDF Vectorizer\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None,\n",
    "                    strip_accents='unicode', analyzer='word',\n",
    "                    ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "# We have not used stopwords argument to remove stopwords in tfidf because the text which we are using are title\n",
    "# of posts and because of this the no. of words are itself less so, there is no point in reducing no. of words further. \n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "# Now transforming to TF-IDF Vectors\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "# saving the trained vectorizer model\n",
    "filename = 'tfidf_vectors.sav'\n",
    "joblib.dump(tfv, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I tried to use count vectorizer with tokenizer first and it was giving a 1% increase in accuracies but during deployment when running the application with gunicorn it is creating a problem in serialization og pickle file of count vectors. I have tried all possible ways, but the problem is unsolved for me. I have tried to pickle the functon alone but this way the problem is not solved. I have also tried to save the pickle file of vectorizer with the tokenizer function but still due to gunicorn, it's throwing error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_vectors.sav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the Count Vectorizer\n",
    "# ctv = CountVectorizer(analyzer='word', tokenizer = wordtokenize, ngram_range=(1, 3))\n",
    "ctv = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "\n",
    "# Same as TF-IDF Vectorizer, here also we have not removed stopwords\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "#Now transforming to Count Vectors\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "\n",
    "\n",
    "# saving the trained vectorizer model\n",
    "filename = 'count_vectors.sav'\n",
    "joblib.dump(ctv, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression\n",
    "### Logistic Regression with TF-IDF Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6177174264468154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgr_tfidf.sav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Logistic Regression model\n",
    "filename = 'lgr_tfidf.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresson with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6219204655674103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgr_count.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Logistic Regression model\n",
    "filename = 'lgr_count.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Naive Bayes with TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5760103459424507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nvb_tfidf.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Naive Bayes model\n",
    "filename = 'nvb_tfidf.sav'\n",
    "joblib.dump(clf, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.611186550274814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nvb_count.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Naive Bayes model\n",
    "filename = 'nvb_count.sav'\n",
    "joblib.dump(clf, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Classifiers are a class of linear classifiers like SVMs, Logistic Regression, etc. By default, it implements\n",
    "SVM classfier with Stochastic Gradient Descent(SGD). The type of classifier can be changed through loss parameter\n",
    "by default it uses \"hinge\" loss. In the cell below, I have mentioned only that combination of SVM and vector which worked best.\n",
    " \n",
    " \n",
    "I have experimented a lot with SMVs and then came to conclusion to use **SGD Classifier with Count Vectors**.\n",
    "Please refer to this [notebook](https://www.kaggle.com/adityakumar01/svm-classifier) or the notebook for svm in this repository for all the experiments regarding SVMs with different vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6217264791464597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sgd_classifier.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intialising the SVM classifier\n",
    "clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30)\n",
    "\n",
    "# fitting the data\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting on validation data\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracies\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the SGD classifer\n",
    "filename = 'sgd_classifier.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "   For XGBoost, I have created a separated notebook where I have experimented with different combinations of XGBoost with different Vectorizer. As XGBoost take a lot of time in training that's why I have not included in this notebook.\n",
    "   Please refer to this [notebook](https://www.kaggle.com/adityakumar01/xgboost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from above we know, Logistic Regression with Count Vectors work best. So, for further we are only going to use Lgoistic Regression with Count Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique(SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising oversampling method SMOTE\n",
    "oversample = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 18486, 3: 16146, 0: 10491, 2: 6570, 1: 3870, 6: 3149, 4: 3144}) Counter({5: 4622, 3: 4036, 0: 2623, 2: 1643, 1: 968, 6: 787, 4: 786})\n"
     ]
    }
   ],
   "source": [
    "# checking distribution of labels before oversampling\n",
    "print(Counter(ytrain), Counter(yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the data into SMOTE for oversampling\n",
    "xtrain_ctv, ytrain = oversample.fit_resample(xtrain_ctv, ytrain)\n",
    "xvalid_ctv, yvalid = oversample.fit_resample(xvalid_ctv, yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 18486, 5: 18486, 6: 18486, 2: 18486, 4: 18486, 1: 18486, 0: 18486}) Counter({3: 4622, 0: 4622, 5: 4622, 6: 4622, 4: 4622, 1: 4622, 2: 4622})\n"
     ]
    }
   ],
   "source": [
    "# checking the ditribution of data after oversampling\n",
    "print(Counter(ytrain), Counter(yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4546578475613525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgr_count_oversample.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Logistic Regression model\n",
    "filename = 'lgr_count_oversample.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
