{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing different libraries\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>dirty_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top comments toi article drop us oil prices</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Top comments on a TOI article about the drop i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hacking networking security 2 books 1 hacking ...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Hacking: Networking and Security (2 Books in 1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zakir khan irfan junejo live instagram session...</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Zakir Khan and Irfan Junejo live Instagram Ses...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cursing quentin tarantino movie</td>\n",
       "      <td>Non-Political</td>\n",
       "      <td>Cursing In A Quentin Tarantino Movie</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          flair  \\\n",
       "0        top comments toi article drop us oil prices  Non-Political   \n",
       "1                                       disappointed       Politics   \n",
       "2  hacking networking security 2 books 1 hacking ...  Non-Political   \n",
       "3  zakir khan irfan junejo live instagram session...  Non-Political   \n",
       "4                    cursing quentin tarantino movie  Non-Political   \n",
       "\n",
       "                                          dirty_text  label  \n",
       "0  Top comments on a TOI article about the drop i...      3  \n",
       "1                                       Disappointed      5  \n",
       "2  Hacking: Networking and Security (2 Books in 1...      3  \n",
       "3  Zakir Khan and Irfan Junejo live Instagram Ses...      3  \n",
       "4               Cursing In A Quentin Tarantino Movie      3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading our dataset\n",
    "train_df = pd.read_csv('../input/midas-task/reddit_data.csv')\n",
    "\n",
    "# dropping rows having null values\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# creating a label column to encode our text labels to no.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df[\"flair\"])\n",
    "train_df[\"label\"] = le.transform(train_df[\"flair\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have two columns of texts \"text\" and \"dirty_text\" which we can use for training. The type of vectorizer       and tokenizer which we will be using further for modelling and creating pipeline will not need cleaned text. The text will be cleaned during vectorization itself, so we will be using \"dirty_text\" column for training instead of \"text\" beacuse it is previously cleaned.\n",
    "    \n",
    "> Note: Actually, I have tried training with both \"dirty_text\" and \"text\" both to validate my above assumption. The results will be shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data and using \"dirty_text\" for training\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_df.dirty_text, train_df.label, \n",
    "                                                  stratify=train_df.label, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61856,)\n",
      "(15465,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for Modelling\n",
    "We will be using two types of vectorizer:\n",
    "1. TfidfVectorizer\n",
    "2. CountVectorizer\n",
    "We will be using our own tokenizer from NLTK.\n",
    "    \n",
    "   ## Models\n",
    "   1. Logistic Regression\n",
    "   2. Multinomial Naive Bayes\n",
    "   3. Support Vector Machines\n",
    "   4. XGBoost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tokenization, we are also using stemming to reduce no. of unique tokens\n",
    "# And we are using Porter Stemmer from NLTK for stemming\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectors.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the TF-IDF Vectorizer\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, tokenizer = tokenize,\n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=None,\n",
    "                    ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "# We have not used stopwords argument to remove stopwords in tfidf because the text which we are using are title\n",
    "# of posts and because of this the no. of words are itself less so, there is no point in reducing no. of words further. \n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "# Now transforming to TF-IDF Vectors\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "# saving the trained vectorizer model\n",
    "filename = 'tfidf_vectors.sav'\n",
    "joblib.dump(tfv, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count_vectors.sav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the Count Vectorizer\n",
    "ctv = CountVectorizer(analyzer='word',tokenizer = tokenize, ngram_range=(1, 3))\n",
    "# Same as TF-IDF Vectorizer, here also we have not removed stopwords\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "\n",
    "#Now transforming to Count Vectors\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "\n",
    "# saving the trained vectorizer model\n",
    "filename = 'count_vectors.sav'\n",
    "joblib.dump(ctv, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression\n",
    "### Logistic Regression with TF-IDF Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6279987067571937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgr_tfidf.sav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Logistic Regression model\n",
    "filename = 'lgr_tfidf.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresson with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6324603944390559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgr_count.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = LogisticRegression(C=1.0)\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Logistic Regression model\n",
    "filename = 'lgr_count.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Naive Bayes with TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5719366311024895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nvb_tfidf.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Naive Bayes model\n",
    "filename = 'nvb_tfidf.sav'\n",
    "joblib.dump(clf, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6106692531522794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nvb_count.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising Logistic Regression model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# training the model\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting with the trained model\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracy on validation data\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the trained Naive Bayes model\n",
    "filename = 'nvb_count.sav'\n",
    "joblib.dump(clf, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have experimented a lot with SMVs and then came to conclusion to use **SGD Classifier with Count Vectors**.\n",
    "Please refer to this [notebook](https://www.kaggle.com/adityakumar01/svm-classifier?scriptVersionId=32678254) for all the experiments regarding SVMs with different vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6314904623343033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sgd_classifier.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD Classifiers are a class of linear classifiers like SVMs, Logistic Regression, etc. By default, it implements\n",
    "# SVM classfier with Stochastic Gradient Descent(SGD). The type of classifier can be changed through loss parameter\n",
    "# by default it uses \"hinge\" loss.\n",
    "clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30)\n",
    "\n",
    "# fitting the data\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "\n",
    "# predicting on validation data\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "\n",
    "# calculating accuracies\n",
    "print('accuracy %s' % accuracy_score(yvalid, predictions))\n",
    "\n",
    "# saving the SGD classifer\n",
    "filename = 'sgd_classifier.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "   For XGBoost, I have created a separated notebook where I have experimented with different combinations of XGBoost with different Vectorizer.\n",
    "   Please refer to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
