{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing different libraries\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nstop_words = stopwords.words('english')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## In this notebook, we are experimenting with differnt SVMs with differnt vectors."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/midas-task/reddit_data.csv')\ntrain_df.dropna(inplace=True)\nle = preprocessing.LabelEncoder()\nle.fit(train_df[\"flair\"])\ntrain_df[\"label\"] = le.transform(train_df[\"flair\"])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train_df.text, train_df.label, \n                                                  stratify=train_df.label, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for tokenization, we are also using stemming to reduce no. of unique tokens\n# And we are using Porter Stemmer from NLTK for stemming\ndef tokenize(text):\n    tokens = word_tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item))\n    return stems","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising the TF-IDF Vectorizer\ntfv = TfidfVectorizer(min_df=3,  max_features=None, tokenizer = tokenize,\n                    strip_accents='unicode', analyzer='word',token_pattern=None,\n                    ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n# We have not used stopwords argument to remove stopwords in tfidf because the text which we are using are title\n# of posts and because of this the no. of words are itself less so, there is no point in reducing no. of words further. \n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\n\n# Now transforming to TF-IDF Vectors\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)\n\n# saving the trained vectorizer model\nfilename = 'tfidf_vectors.sav'\njoblib.dump(tfv, filename)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"['tfidf_vectors.sav']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising the Count Vectorizer\nctv = CountVectorizer(analyzer='word',tokenizer = tokenize, ngram_range=(1, 3))\n# Same as TF-IDF Vectorizer, here also we have not removed stopwords\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\n\n#Now transforming to Count Vectors\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)\n\n# saving the trained vectorizer model\nfilename = 'count_vectors.sav'\njoblib.dump(ctv, filename)","execution_count":10,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"['count_vectors.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Truncated SVD with TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising SVD\nsvd = TruncatedSVD(n_components=200)\n\n# Fitting the TF-IDF vectors\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\n\n# saving the SVD model\nfilename = 'trun_svd.sav'\njoblib.dump(svd, filename)\n\n# saving the Scaler model\nfilename = 'scaler.sav'\njoblib.dump(scl, filename)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['scaler.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## SGD Classifier"},{"metadata":{},"cell_type":"markdown","source":"### SGD with SVD and Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30, tol=None)\n\n# fitting the data\nclf.fit(xtrain_svd_scl, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_svd_scl)\n\n# calculating accuracies\nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SGD classifer\nfilename = 'sgd_classifier.sav'\njoblib.dump(clf, filename)","execution_count":14,"outputs":[{"output_type":"stream","text":"accuracy 0.5131587455544778\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"['sgd_classifier.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### SGD with SVD only"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30, tol=None)\n\n# fitting the data\nclf.fit(xtrain_svd, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_svd)\n\n# calculating accuracies\nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SGD classifer\nfilename = 'sgd_classifier.sav'\njoblib.dump(clf, filename)","execution_count":15,"outputs":[{"output_type":"stream","text":"accuracy 0.4813449725185904\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"['sgd_classifier.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### SGD with TF-IDF vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30)\n\n# fitting the data\nclf.fit(xtrain_tfv, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_tfv)\n\n# calculating accuracies\nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SGD classifer\nfilename = 'sgd_classifier.sav'\njoblib.dump(clf, filename)","execution_count":19,"outputs":[{"output_type":"stream","text":"accuracy 0.5776268994503718\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"['sgd_classifier.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### SGD with Count vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=1e-3, random_state=42, max_iter=30)\n\n# fitting the data\nclf.fit(xtrain_ctv, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_ctv)\n\n# calculating accuracies\nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SGD classifer\nfilename = 'sgd_classifier.sav'\njoblib.dump(clf, filename)","execution_count":20,"outputs":[{"output_type":"stream","text":"accuracy 0.6069188490139024\n","name":"stdout"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"['sgd_classifier.sav']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_ctv.shape, xvalid_tfv","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"((61856, 799833),\n <15465x53649 sparse matrix of type '<class 'numpy.float64'>'\n \twith 166339 stored elements in Compressed Sparse Row format>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Linear SVC with Count vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are Linear Support Vector Classifcation(LinearSVC) in it instead of SVM because SVM is very slow still.\nclf = LinearSVC()\n\n# fitting the data\nclf.fit(xtrain_ctv, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_ctv)\n\n# calculating accuracy \nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SVC classifer\nfilename = 'linear_svc.sav'\njoblib.dump(clf, filename)","execution_count":26,"outputs":[{"output_type":"stream","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 7.63 µs\naccuracy 0.5886841254445522\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"['linear_svc.sav']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### SVC with Count vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are Linear Support Vector Classifcation(LinearSVC) in it instead of SVM because SVM is very slow still.\nclf = SVC(max_iter=1000)\n# fitting the data\nclf.fit(xtrain_ctv, ytrain)\n\n# predicting on validation data\npredictions = clf.predict(xvalid_ctv)\n\n# calculating accuracy \nprint('accuracy %s' % accuracy_score(yvalid, predictions))\n\n# saving the SVC classifer\nfilename = 'linear_svc.sav'\njoblib.dump(clf, filename)","execution_count":33,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n  % self.max_iter, ConvergenceWarning)\n","name":"stderr"},{"output_type":"stream","text":"accuracy 0.31315874555447787\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"['linear_svc.sav']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}